{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf53b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2322162945.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    img.save(\"C:\\Users\\vdubu\\OneDrive\\Bureau\\FISE A3\\Stage\\Livrables\\Résumé\\Logo_celia_bleu_sans_slogan_768xe471.png\", dpi=(300, 300))\u001b[0m\n\u001b[1;37m                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"C:\\Users\\vdubu\\OneDrive\\Bureau\\FISE A3\\Stage\\Livrables\\Résumé\\Logo_celia_bleu_sans_slogan_768x471.png\")\n",
    "img.save(\"C:\\Users\\vdubu\\OneDrive\\Bureau\\FISE A3\\Stage\\Livrables\\Résumé\\Logo_celia_bleu_sans_slogan_768xe471.png\", dpi=(300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe68607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset_path = \"datasets/dataset_light.npy\"\n",
    "loaded_dataset = np.load(dataset_path, allow_pickle=True).item()\n",
    "\n",
    "print(\"Loaded dataset keys:\")\n",
    "for key in loaded_dataset.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "'''print(\"\\nLoaded dataset values:\")\n",
    "for key, value in loaded_dataset.items():\n",
    "    print(f\"- {key}: {value}\")'''\n",
    "\n",
    "# put gain inside a new dictionnary with a key named values\n",
    "new_dataset = loaded_dataset.copy()\n",
    "new_dataset[\"values\"] = {\"gain\": loaded_dataset[\"gain\"]}\n",
    "\n",
    "# add a new key named name with values from 1 to length of gain\n",
    "new_dataset[\"name\"] = [str(i) for i in range(len(loaded_dataset[\"gain\"]))]\n",
    "\n",
    "print(\"\\nNew dataset keys:\")\n",
    "for key in new_dataset.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\nLength of new dataset data:\")\n",
    "print(f\"- {len(new_dataset['data'])}\")\n",
    "\n",
    "# save the new dataset to a file\n",
    "new_dataset_path = \"datasets/dataset_light_correct.npy\"\n",
    "np.save(new_dataset_path, new_dataset, allow_pickle=True)\n",
    "\n",
    "# show the profiles of the new dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "profiles = new_dataset[\"data\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(1):\n",
    "    plt.plot(profiles[i], label=f\"Profile {i+1}\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Profiles of the New Dataset\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2c873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset keys:\n",
      "- values\n",
      "- data\n",
      "- time\n",
      "- name\n",
      "\n",
      "Loaded dataset values:\n",
      "- cutoff\n",
      "- qsup\n",
      "- e95\n",
      "- e99\n",
      "- coil_E\n",
      "- Mgain\n",
      "- gain\n",
      "\n",
      "Loaded dataset coil pitch:\n",
      "- [0.00034766 0.00034624 0.0003448  0.00034335 0.00034187 0.00034037\n",
      " 0.00033886 0.00033734 0.00033581 0.00033427 0.00033274 0.00033121\n",
      " 0.00032971 0.00032823 0.0003268  0.00032543 0.00032412 0.00032289\n",
      " 0.00032176 0.00032075 0.00031986 0.00031911 0.00031852 0.00031808\n",
      " 0.00031782 0.00031773 0.00031782 0.00031808 0.00031852 0.00031912\n",
      " 0.00031988 0.00032077 0.00032178 0.00032289 0.00032407 0.00032531\n",
      " 0.00032657 0.00032783 0.00032907 0.00033026 0.00033139 0.00033244\n",
      " 0.0003334  0.00033425 0.00033499 0.00033563 0.00033618 0.00033663\n",
      " 0.00033701 0.00033734 0.00033763 0.00033792 0.00033821 0.00033854\n",
      " 0.00033892 0.00033937 0.00033992 0.00034057 0.00034132 0.00034217\n",
      " 0.00034313 0.00034419 0.00034533 0.00034653 0.00034779 0.00034907\n",
      " 0.00035036 0.00035165 0.00035291 0.00035415 0.00035533 0.00035648\n",
      " 0.00035758 0.00035864 0.00035966 0.00036067 0.00036168 0.00036268\n",
      " 0.00036371 0.00036477 0.00036587 0.00036701 0.00036819 0.00036942\n",
      " 0.00037068 0.00037196 0.00037326 0.00037455 0.00037582 0.00037706\n",
      " 0.00037824 0.00037937 0.00038042 0.00038139 0.00038227 0.00038306\n",
      " 0.00038376 0.00038437 0.0003849  0.00038535]\n",
      "\n",
      "Loaded dataset time:\n",
      "- [0.         0.00020202 0.00040404 0.00060606 0.00080808 0.0010101\n",
      " 0.00121212 0.00141414 0.00161616 0.00181818 0.0020202  0.00222222\n",
      " 0.00242424 0.00262626 0.00282828 0.0030303  0.00323232 0.00343434\n",
      " 0.00363636 0.00383838 0.0040404  0.00424242 0.00444444 0.00464646\n",
      " 0.00484848 0.00505051 0.00525253 0.00545455 0.00565657 0.00585859\n",
      " 0.00606061 0.00626263 0.00646465 0.00666667 0.00686869 0.00707071\n",
      " 0.00727273 0.00747475 0.00767677 0.00787879 0.00808081 0.00828283\n",
      " 0.00848485 0.00868687 0.00888889 0.00909091 0.00929293 0.00949495\n",
      " 0.00969697 0.00989899 0.01010101 0.01030303 0.01050505 0.01070707\n",
      " 0.01090909 0.01111111 0.01131313 0.01151515 0.01171717 0.01191919\n",
      " 0.01212121 0.01232323 0.01252525 0.01272727 0.01292929 0.01313131\n",
      " 0.01333333 0.01353535 0.01373737 0.01393939 0.01414141 0.01434343\n",
      " 0.01454545 0.01474747 0.01494949 0.01515152 0.01535354 0.01555556\n",
      " 0.01575758 0.0159596  0.01616162 0.01636364 0.01656566 0.01676768\n",
      " 0.0169697  0.01717172 0.01737374 0.01757576 0.01777778 0.0179798\n",
      " 0.01818182 0.01838384 0.01858586 0.01878788 0.0189899  0.01919192\n",
      " 0.01939394 0.01959596 0.01979798 0.02      ]\n",
      "Number of profiles:\n",
      "- 800\n"
     ]
    }
   ],
   "source": [
    "# check all itmes in the dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"datasets/db_june_1_small.npy\"\n",
    "loaded_dataset = np.load(dataset_path, allow_pickle=True).item()\n",
    "\n",
    "print(\"Loaded dataset keys:\")\n",
    "for key in loaded_dataset.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\nLoaded dataset values:\")\n",
    "for key in loaded_dataset[\"values\"].keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "print(\"\\nLoaded dataset coil pitch:\")\n",
    "print(f\"- {loaded_dataset['data']['pitch'][0]}\")\n",
    "\n",
    "print(\"\\nLoaded dataset time:\")\n",
    "print(f\"- {loaded_dataset['time']}\")\n",
    "\n",
    "print(\"Number of profiles:\")\n",
    "print(f\"- {len(loaded_dataset['data']['pitch'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f8278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset_1_path = \"datasets/db_june_2.npy\"\n",
    "dataset_2_path = \"datasets/db_sophie_1.npy\"\n",
    "\n",
    "dataset_1 = np.load(dataset_1_path, allow_pickle=True).item()\n",
    "dataset_2 = np.load(dataset_2_path, allow_pickle=True).item()\n",
    "\n",
    "values_1 = dataset_1[\"values\"]\n",
    "values_2 = dataset_2[\"values\"]\n",
    "values_1[\"sophie\"] = np.zeros(len(values_1[\"e99\"]))\n",
    "values_2[\"sophie\"] = np.ones(len(values_2[\"e99\"]))\n",
    "\n",
    "data_1 = dataset_1[\"data\"]\n",
    "data_2 = dataset_2[\"data\"]\n",
    "time = dataset_1[\"time\"]\n",
    "name_1 = dataset_1[\"name\"]\n",
    "name_2 = dataset_2[\"name\"]\n",
    "\n",
    "# Combine the datasets\n",
    "for key in values_2.keys():\n",
    "    values_1[key] = np.concatenate((values_1[key], values_2[key]))\n",
    "for key in data_1.keys():\n",
    "    data_1[key] = np.concatenate((data_1[key], data_2[key]))\n",
    "combined_dataset = {\n",
    "    \"values\": values_1,\n",
    "    \"data\": data_1,\n",
    "    \"time\": time,\n",
    "    \"name\": np.concatenate((name_1, name_2))\n",
    "}\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_dataset_path = \"datasets/db_june_sophie.npy\"\n",
    "np.save(combined_dataset_path, combined_dataset, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
